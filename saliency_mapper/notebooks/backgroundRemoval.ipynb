{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bJD_FpLestp"
      },
      "source": [
        "# Saliency Mapper\n",
        "\n",
        "> A tool to generate the saliency maps of images using a variety of techniques\n",
        "\n",
        "Code was written by Nicholas M. Synovic, Oscar Yanek, and Rohan Sethi\n",
        "\n",
        "## Optimal Performance\n",
        "\n",
        "For optimal performance, in the *Runtime* tab of Google Collab, click *Change runtime type*, then choose **GPU** from the *Hardware Accelerator* dropdown."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUPl1Y3afhjX"
      },
      "source": [
        "## Upgrade Python `pip` Tool\n",
        "\n",
        "Upgrade the Python `pip` tool to the latest version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTjj0EYGfmxm",
        "outputId": "dfeada01-f30e-4604-970a-b3a9d7576a77"
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbqWSZMQe_B_"
      },
      "source": [
        "## Install Python libaries via `pip`\n",
        "\n",
        "Installed libraries are:\n",
        "\n",
        "- opencv-contrib-python\n",
        "- torch\n",
        "- torchvision\n",
        "- pandas\n",
        "- progress\n",
        "- timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXq-rWDCJd4w",
        "outputId": "78cea49b-95c8-4ca9-a64f-bbd29844412a"
      },
      "outputs": [],
      "source": [
        "%pip install opencv-contrib-python torch torchvision torchaudio pandas progress timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCEpl7l6hKPX"
      },
      "source": [
        "## Import Dependencies "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4vBBhvvfD_u"
      },
      "outputs": [],
      "source": [
        "from os import listdir\n",
        "from os.path import join\n",
        "from pathlib import PurePath\n",
        "\n",
        "import cv2\n",
        "import torch\n",
        "from numpy import ndarray\n",
        "from progress.bar import Bar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUjbgiZ9hVCQ"
      },
      "source": [
        "## Allow Data to be Loaded From Google Drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkdMd79XhalG",
        "outputId": "ad372c16-91c4-4c23-bbf0-53f786604870"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQKDGcMOikM_"
      },
      "source": [
        "## Main Application\n",
        "\n",
        "Initialize variables for program scope "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "spectralSaliency = cv2.saliency.StaticSaliencySpectralResidual_create()\n",
        "fineGrainSaliency = cv2.saliency.StaticSaliencyFineGrained_create()\n",
        "depth_DPTLarge: str = \"DPT_Large\"\n",
        "depth_DPTHybrid: str = \"DPT_Hybrid\"\n",
        "depth_MiDaSsmall: str = \"MiDaS_small\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simple Directory Reader\n",
        "\n",
        "when giving the program a dataset instead of singular image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def readDirectory(dir: str) -> list:\n",
        "    files: list = listdir(dir)\n",
        "    filepaths: list = [join(dir, f) for f in files]\n",
        "    return filepaths"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EstimateDepth Main Logic\n",
        "\n",
        "Determines desired training model from the arguments. Uses loading bar to parse through imagePaths List, performs transformation on each image and outputs the result to a new folder. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def estimateDepth(imagePaths: list, modelType: str, outputFolder: str = \"data\") -> None:\n",
        "    midas = torch.hub.load(\"intel-isl/MiDaS\", modelType)\n",
        "    midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
        "\n",
        "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "    midas.to(device)\n",
        "    if modelType == \"DPT_Large\" or modelType == \"DPT_Hybrid\":\n",
        "        transform = midas_transforms.dpt_transform\n",
        "    else:\n",
        "        transform = midas_transforms.small_transform\n",
        "\n",
        "    with Bar(f\"Estimating depth with {modelType}...\", max=(len(imagePaths))) as bar:\n",
        "        imagePath: str\n",
        "        for imagePath in imagePaths:\n",
        "            imageName: str = (\n",
        "                PurePath(imagePath).with_suffix(\"\").name\n",
        "                + f'_{modelType.replace(\"_\", \"-\")}.jpg'\n",
        "            )\n",
        "            outputPath: str = join(outputFolder, imageName)\n",
        "\n",
        "            image: ndarray = cv2.imread(imagePath)\n",
        "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            input_batch = transform(image).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                prediction = midas(input_batch)\n",
        "\n",
        "                prediction = torch.nn.functional.interpolate(\n",
        "                    prediction.unsqueeze(1),\n",
        "                    size=image.shape[:2],\n",
        "                    mode=\"bicubic\",\n",
        "                    align_corners=False,\n",
        "                ).squeeze()\n",
        "\n",
        "            output = prediction.cpu().numpy()\n",
        "            cv2.imwrite(outputPath, output)\n",
        "            bar.next()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ComputeSpectralSaliency Main Logic\n",
        "\n",
        "Creates output folder and path, takes input imagePath and creates a saliency map for the image using the spectral residual approach. Starting from the principle of natural image statistics, this method simulate the behavior of pre-attentive visual search. The algorithm analyze the log spectrum of each image and obtain the spectral residual. Then transform the spectral residual to spatial domain to obtain the saliency map, which suggests the positions of proto-objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def computeSpectralSaliency(imagePath: str, outputFolder: str = \"data\") -> None:\n",
        "    imageName: str = PurePath(imagePath).with_suffix(\"\").name + \"_spectralResidual.jpg\"\n",
        "    outputPath: str = join(outputFolder, imageName)\n",
        "    image: ndarray = cv2.imread(imagePath)\n",
        "    (success, saliencyMap) = spectralSaliency.computeSaliency(image)\n",
        "    saliencyMap: ndarray = (saliencyMap * 255).astype(\"uint8\")\n",
        "    cv2.imwrite(outputPath, saliencyMap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ComputeFineGrainSaliency Main Logic\n",
        "\n",
        "Creates output folder and path, takes input imagePath and creates a saliency map for the image using the fine grained approach. This method calculates saliency based on center-surround differences. High resolution saliency maps are generated in real time by using integral images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def computeFineGrainSaliency(imagePath: str, outputFolder: str = \"data\") -> None:\n",
        "    imageName: str = PurePath(imagePath).with_suffix(\"\").name + \"_fineGrain.jpg\"\n",
        "    outputPath: str = join(outputFolder, imageName)\n",
        "    image: ndarray = cv2.imread(imagePath)\n",
        "    (success, saliencyMap) = fineGrainSaliency.computeSaliency(image)\n",
        "    saliencyMap: ndarray = (saliencyMap * 255).astype(\"uint8\")\n",
        "    cv2.imwrite(outputPath, saliencyMap)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Triangle Threshold Background Removal\n",
        "\n",
        "Blurs image to reduce noise. Bins the pixels between 1-5. Creates a single channel greyscale for thresholding. Performs Otsu Thresholding and extracts the background. Uses binary thershold to create an all white background. Converts black and white back into 3 channel greyscale. Performs Triangle thresholding and extracts the foreground. Uses TOZERO_INV to keep some detail of the foreground. Combines the background and foreground to make final image. returns final image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def triangleBackgroundRemoval(imagePath: str, outputFolder: str = \"data\") -> None:\n",
        "    imageName: str = PurePath(imagePath).with_suffix(\"\").name + \"_triangleBackgroundRemoval.jpg\"\n",
        "    outputPath: str = join(outputFolder, imageName)\n",
        "    bins: ndarray = numpy.array([0, 51, 102, 153, 204, 255])\n",
        "\n",
        "    image: ndarray = cv2.imread(imagePath)\n",
        "    blurredImage: ndarray = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "    blurredImage[:, :, :] = numpy.digitize(blurredImage[:, :, :], bins, right=True) * 51\n",
        "    grayBlurredImage: ndarray = cv2.cvtColor(blurredImage, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    ret, foreground = cv2.threshold(\n",
        "        grayBlurredImage, 0, 255, cv2.THRESH_TOZERO_INV + cv2.THRESH_TRIANGLE\n",
        "    )\n",
        "\n",
        "    foreground[foreground > 0] = 255    # Convert from grayscale to black and white\n",
        "\n",
        "    cv2.imwrite(outputPath, foreground)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Otsu Threshold Background Removal\n",
        "\n",
        "Blurs image to reduce noise. Bins the pixels between 1-5. Creates a single channel greyscale for thresholding. Performs Otsu Thresholding and extracts the background. Uses binary thershold to create an all white background. Converts black and white back into 3 channel greyscale. Performs Otsu thresholding and extracts the foreground. Uses TOZERO_INV to keep some detail of the foreground. Combines the background and foreground to make final image. returns final image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def otsuThreshold(imagePath: str, outputFolder: str = \"data\") -> None:\n",
        "    imageName: str = PurePath(imagePath).with_suffix(\"\").name + \"_otsu.jpg\"\n",
        "    outputPath: str = join(outputFolder, imageName)\n",
        "    bins: ndarray = numpy.array([0, 51, 102, 153, 204, 255])\n",
        "\n",
        "    image: ndarray = cv2.imread(imagePath)\n",
        "    blurredImage: ndarray = cv2.GaussianBlur(image, (5, 5), 0)\n",
        "\n",
        "    blurredImage[:, :, :] = numpy.digitize(blurredImage[:, :, :], bins, right=True) * 51\n",
        "    grayBlurredImage: ndarray = cv2.cvtColor(blurredImage, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    ret, foreground = cv2.threshold(\n",
        "        grayBlurredImage, 0, 255, cv2.THRESH_TOZERO_INV + cv2.THRESH_OTSU\n",
        "    )\n",
        "\n",
        "    foreground[foreground > 0] = 255    # Convert from grayscale to black and white\n",
        "\n",
        "    cv2.imwrite(outputPath, foreground)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basic Thresholding Background Removal\n",
        "\n",
        "First converts to greyscale. Performs truncate threshold to get baseline. Extracts binary threshold using the baseline for the background and foreground. Updates foreground with bitwise_and to extract real foreground. Converts black and white back into 3 channel greyscale. Combines the background and foreground to obtain our final image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def basicBackgroundRemoval(imagePath: str, outputFolder: str = \"data\") -> None:\n",
        "    imageName: str = PurePath(imagePath).with_suffix(\"\").name + \"_basicBackgroundRemoval.jpg\"\n",
        "    outputPath: str = join(outputFolder, imageName)\n",
        "\n",
        "    image: ndarray = cv2.imread(imagePath)\n",
        "\n",
        "    grayImage: ndarray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    baseline: ndarray = cv2.threshold(grayImage,127,255,cv2.THRESH_TRUNC)[1]\n",
        "    foreground: ndarray = cv2.threshold(baseline,126,255,cv2.THRESH_BINARY_INV)[1]\n",
        "\n",
        "    foreground[foreground > 0] = 255    # Convert from grayscale to black and white\n",
        "\n",
        "    cv2.imwrite(outputPath, foreground)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hue Saturation Value\n",
        "\n",
        "Converts image from BGR to HSV. Takes saturation and removes any values that are less than half creating the saturation mask. Increases the brightness of the image and then mods by 255 . Extracts any value above 127 to be a part of the value mask. Combines the two masks into unified foreground. Casts back into 8-bit integer. Inverts foreground to get background in uint8. Converts background back into BGR. Applies foreground map to original image. Combines foreground and background.\n",
        "\n",
        "Documentation: https://docs.opencv.org/4.x/df/d9d/tutorial_py_colorspaces.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def hsvBackgroundRemoval(imagePath: str, outputFolder: str = \"data\") -> None:\n",
        "    imageName: str = PurePath(imagePath).with_suffix(\"\").name + \"_hsvBackgroundRemoval.jpg\"\n",
        "    outputPath: str = join(outputFolder, imageName)\n",
        "\n",
        "    image: ndarray = cv2.imread(imagePath)\n",
        "\n",
        "    hsvImage = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    saturation: hsvImage = hsvImage[:,:,1]\n",
        "    saturation = numpy.where(saturation < 127, 0, 1)\n",
        "\n",
        "    colorValue: ndarray = (hsvImage[:,:,2] + 127) % 255\n",
        "    colorValue = numpy.where(colorValue > 127, 1, 0)\n",
        "\n",
        "    foreground = numpy.where(saturation + colorValue > 0, 1, 0).astype(numpy.uint8)\n",
        "\n",
        "    foreground[foreground > 0] = 255    # Convert from grayscale to black and white\n",
        "\n",
        "    cv2.imwrite(outputPath, foreground)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Combined Main Method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "oTH2X0mhhbz1",
        "outputId": "70cab19a-d784-4a94-d8c1-87dd16f1f0cc"
      },
      "outputs": [],
      "source": [
        "def main() -> None:\n",
        "    # dirImage = input(\"image dir path is: \")\n",
        "    # dirOut = input(\"output image dir path is: \")\n",
        "    # imagePaths: list = readDirectory(dir=dirImage)\n",
        "    imagePath: str = input(\"Enter path: \")\n",
        "    imagePaths: list = [imagePath]\n",
        "    dirOut = \"data\"\n",
        "    with Bar(\n",
        "        \"Creating saliency maps of PascalVOC images...\", max=len(imagePaths)\n",
        "    ) as bar:\n",
        "        imagePath: str\n",
        "        for imagePath in imagePaths:\n",
        "            computeSpectralSaliency(imagePath)\n",
        "            computeFineGrainSaliency(imagePath)\n",
        "            bar.next()\n",
        "\n",
        "    \n",
        "    image: ndarray = cv2.imread(imagePath)\n",
        "    showimage(bgremove1(image))\n",
        "    showimage(bgremove2(image))\n",
        "    showimage(bgremove3(image))\n",
        "\n",
        "    estimateDepth(imagePaths, depth_DPTHybrid)\n",
        "    estimateDepth(imagePaths, depth_DPTLarge)\n",
        "    estimateDepth(imagePaths, depth_MiDaSsmall)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "7d6993cb2f9ce9a59d5d7380609d9cb5192a9dedd2735a011418ad9e827eb538"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
